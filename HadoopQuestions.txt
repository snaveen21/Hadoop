1. What is Big Data?
Any data that cannot be stored into traditional RDBMS is termed as Big Data. 
As we know most of the data that we use today has been generated in the past 20 years. 
And this data is mostly unstructured or semi structured in nature

2. What do the four V’s of Big Data denote?
IBM has a nice, simple explanation for the four critical features of big data:
a) Volume –Scale of data
b) Velocity –Different forms of data
c) Variety –Analysis of streaming data
d) Veracity –Uncertainty of data

3. Differentiate between Structured and Unstructured data.

Data which can be stored in traditional database systems in the form of rows and columns, 
for example the online purchase transactions can be referred to as Structured Data. 
Data which can be stored only partially in traditional database systems, for example,
 data in XML records can be referred to as semi structured data. Unorganized and 
 raw data that cannot be categorized as semi structured or structured data is referred to as unstructured data. 
 Facebook updates, Tweets on Twitter, Reviews, web logs, etc. are all examples of unstructured data.
 
 
 4. On what concept the Hadoop framework works?
Hadoop Framework works on the following two core components-
1)HDFS – Hadoop Distributed File System is the java based file system for scalable and reliable storage of large datasets. 
Data in HDFS is stored in the form of blocks and it operates on the Master Slave Architecture.

2)Hadoop MapReduce-This is a java based programming paradigm of Hadoop framework that provides scalability across various Hadoop clusters. 
MapReduce distributes the workload into various tasks that can run in parallel. Hadoop jobs perform 2 separate tasks- job. 
The map job breaks down the data sets into key-value pairs or tuples. The reduce job then takes the output of the map job and 
combines the data tuples to into smaller set of tuples. The reduce job is always performed after the map job is executed.


5. What is Hadoop streaming?
Hadoop distribution has a generic application programming interface for writing Map and Reduce jobs in any desired programming 
language like Python, Perl, Ruby, etc. This is referred to as Hadoop Streaming. Users can create and run jobs with any kind of 
shell scripts or executable as the Mapper or Reducers.


6. What are the most commonly defined input formats in Hadoop?

The most common Input Formats defined in Hadoop are:
Text Input Format- This is the default input format defined in Hadoop.
Key Value Input Format- This input format is used for plain text files wherein the files are broken down into lines.
Sequence File Input Format- This input format is used for reading files in sequence.


7. What is a block and block scanner in HDFS?

Block - The minimum amount of data that can be read or written is generally referred to as a “block” in HDFS. The default size of a block in HDFS is 64MB.

Block Scanner - Block Scanner tracks the list of blocks present on a DataNode and verifies them to find any kind of checksum errors. Block Scanners use a throttling mechanism to reserve disk bandwidth on the datanode.



8. What is the port number for NameNode, Task Tracker and Job Tracker?

NameNode 50070

Job Tracker 50030

Task Tracker 50060


9. How can you overwrite the replication factors in HDFS?

The replication factor in HDFS can be modified or overwritten in 2 ways-

1)Using the Hadoop FS Shell, replication factor can be changed per file basis using the below command-

$hadoop fs –setrep –w 2 /my/test_file (test_file is the filename whose replication factor will be set to 2)

2)Using the Hadoop FS Shell, replication factor of all files under a given directory can be modified using the below command-

3)$hadoop fs –setrep –w 5 /my/test_dir (test_dir is the name of the directory and all the files in this directory will have a replication factor set to 5)




10. What is a rack awareness and on what basis is data stored in a rack?

All the data nodes put together form a storage area i.e. the physical location of the data nodes is referred to as Rack in HDFS. The rack information i.e. the rack id of each data node is acquired by the NameNode. The process of selecting closer data nodes depending on the rack information is known as Rack Awareness.

The contents present in the file are divided into data block as soon as the client is ready to load the file into the hadoop cluster. After consulting with the NameNode, client allocates 3 data nodes for each data block. For each data block, there exists 2 copies in one rack and the third copy is present in another rack. This is generally referred to as the Replica Placement Policy.


11. Explain the usage of Context Object.

Context Object is used to help the mapper interact with other Hadoop systems. Context Object can be used for updating counters, to report the progress and to provide any application level status updates. ContextObject has the configuration details for the job and also interfaces, that helps it to generating the output.



12. What are the core methods of a Reducer?

The 3 core methods of a reducer are –

1)setup () – This method of the reducer is used for configuring various parameters like the input data size, distributed cache, heap size, etc.

Function Definition- public void setup (context)

2)reduce () it is heart of the reducer which is called once per key with the associated reduce task.

Function Definition -public void reduce (Key,Value,context)

3)cleanup () - This method is called only once at the end of reduce task for clearing all the temporary files.




13. Explain about the partitioning, shuffle and sort phase

Shuffle Phase-Once the first map tasks are completed, the nodes continue to perform several other map tasks and also exchange the intermediate outputs with the reducers as required. This process of moving the intermediate outputs of map tasks to the reducer is referred to as Shuffling.

Sort Phase- Hadoop MapReduce automatically sorts the set of intermediate keys on a single node before they are given as input to the reducer.

Partitioning Phase-The process that determines which intermediate keys and value will be received by each reducer instance is referred to as partitioning. The destination partition is same for any key irrespective of the mapper instance that generated it.




14. How to write a custom partitioner for a Hadoop MapReduce job?

Steps to write a Custom Partitioner for a Hadoop MapReduce Job-

A new class must be created that extends the pre-defined Partitioner Class.
getPartition method of the Partitioner class must be overridden.
The custom partitioner to the job can be added as a config file in the wrapper which runs Hadoop MapReduce or the custom partitioner can be added to the job by using the set method of the partitioner class.



15. What is the relationship between Job and Task in Hadoop?

A single job can be broken down into one or many tasks in Hadoop.

16. Is it important for Hadoop MapReduce jobs to be written in Java?

It is not necessary to write Hadoop MapReduce jobs in java but users can write MapReduce jobs in any desired programming language like Ruby, Perl, Python, R, Awk, etc. through the Hadoop Streaming API.



17. What is the process of changing the split size if there is limited storage space on Commodity Hardware?

If there is limited storage space on commodity hardware, the split size can be changed by implementing the “Custom Splitter”. The call to Custom Splitter can be made from the main method.



18. What are the primary phases of a Reducer?

The 3 primary phases of a reducer are –

1)Shuffle

2)Sort

3)Reduce


19. Can reducers communicate with each other?

Reducers always run in isolation and they can never communicate with each other as per the Hadoop MapReduce programming paradigm.


20) What do you mean by a bag in Pig?

Collection of tuples is referred as a bag in Apache Pig

21) Does Pig support multi-line commands?

Yes


22)Explain the need for MapReduce while programming in Apache Pig.

23)Explain about co-group in Pig.

24)Explain about the BloomMapFile.

25)Differentiate between Hadoop MapReduce and Pig

26)What is the usage of foreach operation in Pig scripts?

27)Explain about the different complex data types in Pig.

28)What Flatten does in Pig?


29)Explain about the different types of join in Hive.

30)How can you configure remote metastore mode in Hive?

31)Explain about the SMB Join in Hive.

32)Is it possible to change the default location of Managed Tables in Hive, if so how?

33)How data transfer happens from Hive to HDFS?

34)How can you connect an application, if you run Hive as a server?

35)What does the overwrite keyword denote in Hive load statement?

36)What is SerDe in Hive? How can you write yourown customer SerDe?

37)In case of embedded Hive, can the same metastore be used by multiple users?

38.How you will add/delete a node in existing cluster ?

a.add the host name/ip address in dfs.hosts/slaves file and refresh the cluster with $hadoop dfsamin -refreshNodes
b.add the host ip address to dfs.hosts.exclude/remove the entry from slaves file and refresh the cluster with $hadoop dfsamin -refreshNodes

39.How to format hdfs?
a. $hadoop namenode -format
Format sld be done only once at the initial stage of cluster setup

40.what is dfsadmin commnd in hadoop?
We can do some administration operation in this like -reports,-safemode enter|get|leave|wait,-refreshNodes,

41.How will you come to know if a data node failed to start?
via hdfs web ui .we can see no of decomissioned nodes and need to rebalance the cluster now

42.What happens if name node fails and what action need to take?
Entire hdfs will be down and we need to restrt the name node after copying the fsimage and editlog from secondary name node.
we can find the path in hdfs-site-xml in dfs.namenode.name tag


43.how do we set logging level in hadoop?

in log4j.properties or in hadoop-env.sh  file .
hadoop.root.logger=info,console,warn etc


44.how can we verify the status and health of cluster?
By hdfs ui ie. http://localhost:50070 or by $hadoop dfsadmin -report


45.How can we restrict the override of configuration ?
by adding <final>true</true>


46.what are the compression technique in hadoop and which is the best one and why?
gzip,bzip2,lzo,snappy.gzip is the best compression format.

47.how to see compressed file via hdfs command?

hadoop -fs text


48. how to change block size?

hadoop fs -Ddfs.block.size=1048576

49.why blocks are so large in hadoop?

50.what is check point?
51.what is sequence file and how it is diff from text file?
52.what is safe mode in hadoop?
53.what is the limitation of  sequence file?
it supports only java
54.what are avro files?
55.can avro file read in ruby api?
yes
56.where does the schema of avro file store if the file is transfered from one host to another?
in same file in header section
57.how to handle small file in hdfs?
merging into seq/avro file or archieve theminto har files.
58.what is delegation token in hadoop ?
59.what is fsck command?
If all replicas of one or more blocks of a file become unavailable, a file is considered corrupt and any attempt to access this file will lead to exception.
To check health of hadoop filesystem like Linux hadoop has "fsck" command. 
60.can we append data to an existing file in hdfs?
A) Yes by command $ hdfs dfs -appendToFile … Appends single src, or multiple srcs from local file system to the destination file system. Also reads input from stdin and appends to destination file system.
61.what is speculative execution?
62.what is distributed cache?
63.wrk flow of mr job?
map>combiner>suffle>partitioner>reducer
64.diff between nmapside and reduce side join?
65.what is map reduce chaining?
66.how will you pass parameters in mapper and reducer?
67.how to create custom key and value in mr?
A class which implements WritableComparable interface can be used for Key and Value. That means your new custom class will be Writable and Comparable also.
The super interfaces of WritableComparable are Writable and Comparable.
68.How to create custom input format?
class need to extend fileinputformat
69.how can we run reducer without mapper?
A) Yes in this Identity mapper will be run in the back ground to copy the input to reducer
70.Whether mapper and reducer tasks run in parallel? If no, why see some times as (map 80%,reduce 10%)?
A) No, its due to data copy phase.
71. How will you setup a custom counter to detect bad records in the input? 
A) context.getcounter.enumvalue
72.15. How will you schedule mapreduce Jobs?
A) Through Oozie or Azkaban
73.16. what is combiner?Tell me one scenario where it is not suitable? 
A) for aggregate functions
74.19. For a failed mapreduce job how will trace for the root cause 
A) Yarn WEB UI ? logs –> Userlogs ? Application ID container ? Syserr/syslog/
75. 20. What will you do if a mapreduce job failed with Java heap space error message?
A) In HADOOP_CLIENT_OPTS or JAVA_CHILD_OPTS increase Xmx property
76. 21. How many map tasks & reduce tasks will run on each datanode by default 
A) 2 map tasks and 1 reduce task
77. 25. What is input split, input format and record reader in Mapreduce programming?
78.28. Can we create multi level directory structure (year/month/date) in Mapreduce based on the input data? 
A) yes by using multipleoutputs
79.29. What is the relation between TextOutputFormat and KeyValueTextInputFormat? 
A) second one is used to read the files created by first one
80. 30. What is LazyOutpuFormat in Mapreduce and why do we need it? 
A) creates output files if data is present
81. 31. How do we prevent file splitting in Mapreduce ? 
A) by returning false from isSplittable method on our custom InputFormat Class
82. 32. What is the difference between Writable and WritableComparable interfaces? And what is sufficient for value type in MR job? 
A) writable
83. 35. What are IdentityMapper & IdentityReducer classes?
84. 44. If a map task is failed once during mapreduce job execution will job fail immediately? 
A) No it will try restarting the tasks upto max attempts allowed on map/reduce tasks, by default it is 4
85.how to write udf in java for pig?
1.public class Ucfirst extends EvalFunc
2.public String exec(Tuple input) throws IOException 
3.register a jar>>>REGISTER .jar
4.define ur class i.e define xyz com.sixdee.UdfClass()
86. 1. How will load a file into pig?
87.2. What are the complex data types in pig?
88. 3. What is outer bag?
89. 6. What is the difference between inner bag and outer bag?
90. 7. What is a tuple?
91. 8. What is the difference between FOREACH and FILTER?
92. 9. What is the difference between local mode and mapreduce mode?
93. 10. What is the difference between GROUP BY and JOIN BY in Pig?
94. 11. How many reduce tasks will be run if we specify both GROUP BY and ORDER BY clauses in the same pig script?
95. 12. What is DISTINCT operator?
96. 13. Difference between UNION, JOIN and CROSS ?
97.14. How do we sort records in descending order in a dataset in Pig? (ORDER DESC/ASC)
98. 15. What is the difference between GROP and COGROUP?
99. 16. What is the difference between STORE and DUMP commands?
100. 17. How will you debug a pig script ? 
set debug on
101. 18. Can we run basic Hadoop fs commands in Grunt shell? 
yes
102. 19. Can we run Unix shell commands from Grunt shell itself ? 
by using sh comand
103. 20. Can we submit pig scripts in batch mode from grunt shell?
yes by using run/exe command
104. 22. What are diagnostic operators in Pig?
105. 23. What is the difference between EXPLAIN, ILLUSTRATE and DESCRIBE?
106. 24. How do we access a custome UDF function created in Pig? 
by using register and assign
107. 25. What is DIFF function in pig?
108. 26. Can we do random sampling from a large dataset in pig? 
sample
109. 27. How can we divide records of a single dataset into multiple datasets by using any criteria like country wise? 
SPLIT command
110.28. What is the difference between COUNT and COUNT_START functions in pig? 
A) COUNT_START includes null values also in counting whereas COUNT will not
111. 29. What are PigStorage & HBaseStorage ?
112. 30. What is the use of LIMIT in pig?
113. 31. What is the difference between Mapreduce and Pig and can we use Pig in all scenarios where we can write MR jobs? 
no
114. What is a JobTracker in Hadoop? How many instances of JobTracker run on a Hadoop Cluster?
JobTracker is the daemon service for submitting and tracking MapReduce jobs in Hadoop. There is only One Job Tracker process run on any hadoop cluster. Job Tracker runs on its own JVM process. In a typical production cluster its run on a separate machine. Each slave node is configured with job tracker node location. The JobTracker is single point of failure for the Hadoop MapReduce service. If it goes down, all running jobs are halted.
115.JobTracker role in Hadoop ?
Client applications submit jobs to the Job tracker.
The JobTracker talks to the NameNode to determine the location of the data
The JobTracker locates TaskTracker nodes with available slots at or near the data
The JobTracker submits the work to the chosen TaskTracker nodes.
The TaskTracker nodes are monitored. If they do not submit heartbeat signals often enough, they are deemed to have failed and the work is scheduled on a different TaskTracker.
A TaskTracker will notify the JobTracker when a task fails. The JobTracker decides what to do then: it may resubmit the job elsewhere, it may mark that specific record as something to avoid, and it may may even blacklist the TaskTracker as unreliable.
When the work is completed, the JobTracker updates its status.

115.How JobTracker schedules a task?
he TaskTrackers send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These message also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated. When the JobTracker tries to find somewhere to schedule a task within the MapReduce operations, it first looks for an empty slot on the same server that hosts the DataNode containing the data, and if not, it looks for an empty slot on a machine in the same rack.

116.What is a Task Tracker in Hadoop? How many instances of TaskTracker run on a Hadoop Cluster
A TaskTracker is a slave node daemon in the cluster that accepts tasks (Map, Reduce and Shuffle operations) from a JobTracker. There is only One Task Tracker process run on any hadoop slave node. Task Tracker runs on its own JVM process. Every TaskTracker is configured with a set of slots, these indicate the number of tasks that it can accept. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. The TaskTracker monitors these task instances, capturing the output and exit codes. When the Task instances finish, successfully or not, the task tracker notifies the JobTracker. The TaskTrackers also send out heartbeat messages to the JobTracker, usually every few minutes, to reassure the JobTracker that it is still alive. These message also inform the JobTracker of the number of available slots, so the JobTracker can stay up to date with where in the cluster work can be delegated

117.What is a Task instance in Hadoop? Where does it run?
Task instances are the actual MapReduce jobs which are run on each slave node. The TaskTracker starts a separate JVM processes to do the actual work (called as Task Instance) this is to ensure that process failure does not take down the task tracker. Each Task Instance runs on its own JVM process. There can be multiple processes of task instance running on a slave node. This is based on the number of slots configured on task tracker. By default a new task instance JVM process is spawned for a task.

118.How many Daemon processes run on a Hadoop system?
Hadoop is comprised of five separate daemons. Each of these daemon run in its own JVM. Following 3 Daemons run on Master nodes NameNode - This daemon stores and maintains the metadata for HDFS. Secondary NameNode - Performs housekeeping functions for the NameNode. JobTracker - Manages MapReduce jobs, distributes individual tasks to machines running the Task Tracker. Following 2 Daemons run on each Slave nodes DataNode - Stores actual HDFS data blocks. TaskTracker - Responsible for instantiating and monitoring individual Map and Reduce tasks.

119.What is configuration of a typical slave node on Hadoop cluster? How many JVMs run on a slave node?
Single instance of a Task Tracker is run on each Slave node. Task tracker is run as a separate JVM process.
Single instance of a DataNode daemon is run on each Slave node. DataNode daemon is run as a separate JVM process.
One or Multiple instances of Task Instance is run on each slave node. Each task instance is run as a separate JVM process. The number of Task instances can be controlled by configuration. Typically a high end machine is configured to run more task instances.

120.What is the difference between HDFS and NAS ?
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. Following are differences between HDFS and NAS
In HDFS Data Blocks are distributed across local drives of all machines in a cluster. Whereas in NAS data is stored on dedicated hardware.
HDFS is designed to work with MapReduce System, since computation are moved to data. NAS is not suitable for MapReduce since data is stored seperately from the computations.
HDFS runs on a cluster of machines and provides redundancy usinga replication protocal. Whereas NAS is provided by a single machine therefore does not provide data redundancy.

121.How NameNode Handles data node failures?
NameNode periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode. When NameNode notices that it has not recieved a hearbeat message from a data node after a certain amount of time, the data node is marked as dead. Since blocks will be under replicated the system begins replicating the blocks that were stored on the dead datanode. The NameNode Orchestrates the replication of data blocks from one datanode to another. The replication data transfer happens directly between datanodes and the data never passes through the namenode.

122.Does MapReduce programming model provide a way for reducers to communicate with each other? In a MapReduce job can a reducer communicate with another reducer?
Nope, MapReduce programming model does not allow reducers to communicate with each other. Reducers run in isolation.

123.Can I set the number of reducers to zero?
yes

124.Where is the Mapper Output (intermediate kay-value data) stored ?
The mapper output (intermediate data) is stored on the Local file system (NOT HDFS) of each individual mapper nodes. This is typically a temporary directory location which can be setup in config by the hadoop administrator. The intermediate data is cleaned up after the Hadoop Job completes.

125.What are combiners? When should I use a combiner in my MapReduce Job?
Combiners are used to increase the efficiency of a MapReduce program. They are used to aggregate intermediate map output locally on individual mapper outputs. Combiners can help you reduce the amount of data that needs to be transferred across to the reducers. You can use your reducer code as a combiner if the operation performed is commutative and associative. The execution of combiner is not guaranteed, Hadoop may or may not execute a combiner. Also, if required it may execute it more then 1 times. Therefore your MapReduce jobs should not depend on the combiners execution.

126.What is Writable & WritableComparable interface?
org.apache.hadoop.io.Writable is a Java interface. Any key or value type in the Hadoop Map-Reduce framework implements this interface. Implementations typically implement a static read(DataInput) method which constructs a new instance, calls readFields(DataInput) and returns the instance.
org.apache.hadoop.io.WritableComparable is a Java interface. Any type which is to be used as a key in the Hadoop Map-Reduce framework should implement this interface. WritableComparable objects can be compared to each other using Comparators.

127.What is the Hadoop MapReduce API contract for a key and value Class?
The Key must implement the org.apache.hadoop.io.WritableComparable interface.
The value must implement the org.apache.hadoop.io.Writable interface.

128.What is a IdentityMapper and IdentityReducer in MapReduce ?
org.apache.hadoop.mapred.lib.IdentityMapper Implements the identity function, mapping inputs directly to outputs. If MapReduce programmer do not set the Mapper Class using JobConf.setMapperClass then IdentityMapper.class is used as a default value.
org.apache.hadoop.mapred.lib.IdentityReducer Performs no reduction, writing all input values directly to the output. If MapReduce programmer do not set the Reducer Class using JobConf.setReducerClass then IdentityReducer.class is used as a default value.

129.What is the meaning of speculative execution in Hadoop? Why is it important?
Speculative execution is a way of coping with individual Machine performance. In large clusters where hundreds or thousands of machines are involved there may be machines which are not performing as fast as others. This may result in delays in a full job due to only one machine not performaing well. To avoid this, speculative execution in hadoop can run multiple copies of same map or reduce task on different slave nodes. The results from first node to finish are used.

120.When is the reducers are started in a MapReduce job?
In a MapReduce job reducers do not start executing the reduce method until the all Map jobs have completed. Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The programmer defined reduce method is called only after all the mappers have finished.

121.If reducers do not start before all mappers finish then why does the progress on MapReduce job shows something like Map(50%) Reduce(10%)? Why reducers progress percentage is displayed when mapper is not finished yet?
Reducers start copying intermediate key-value pairs from the mappers as soon as they are available. The progress calculation also takes in account the processing of data transfer which is done by reduce process, therefore the reduce progress starts showing up as soon as any intermediate key-value pair for a mapper is available to be transferred to reducer. Though the reducer progress is updated still the programmer defined reduce method is called only after all the mappers have finished.

122.What is HDFS ? How it is different from traditional file systems?
HDFS, the Hadoop Distributed File System, is responsible for storing huge data on the cluster. This is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant.
HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware.
HDFS provides high throughput access to application data and is suitable for applications that have large data sets.
HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files.

123.What is HDFS Block size? How is it different from traditional file system block size?
In HDFS data is split into blocks and distributed across multiple nodes in the cluster. Each block is typically 64Mb or 128Mb in size. Each block is replicated multiple times. Default is to replicate each block three times. Replicas are stored on different nodes. HDFS utilizes the local file system to store each HDFS block as a separate file. HDFS Block size can not be compared with the traditional file system block size.

124.What is a NameNode? How many instances of NameNode run on a Hadoop Cluster?
The NameNode is the centerpiece of an HDFS file system. It keeps the directory tree of all files in the file system, and tracks where across the cluster the file data is kept. It does not store the data of these files itself. There is only One NameNode process run on any hadoop cluster. NameNode runs on its own JVM process. In a typical production cluster its run on a separate machine. The NameNode is a Single Point of Failure for the HDFS Cluster. When the NameNode goes down, the file system goes offline. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives.

125.What is a DataNode? How many instances of DataNode run on a Hadoop Cluster?
A DataNode stores data in the Hadoop File System HDFS. There is only One DataNode process run on any hadoop slave node. DataNode runs on its own JVM process. On startup, a DataNode connects to the NameNode. DataNode instances can talk to each other, this is mostly during replicating data.

126.How the Client communicates with HDFS?
The Client communication to HDFS happens using Hadoop HDFS API. Client applications talk to the NameNode whenever they wish to locate a file, or when they want to add/copy/move/delete a file on HDFS. The NameNode responds the successful requests by returning a list of relevant DataNode servers where the data lives. Client applications can talk directly to a DataNode, once the NameNode has provided the location of the data.

127.How the HDFS Blocks are replicated?
HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. HDFS uses rack-aware replica placement policy. In default configuration there are total 3 copies of a datablock on HDFS, 2 copies are stored on datanodes on same rack and 3rd copy on a different rack.

128.Name the most common Input Formats defined in Hadoop? Which one is default?
TextInputFormat
- KeyValueInputFormat
- SequenceFileInputFormat
TextInputFormat is the Hadoop default.

129.What is the difference between TextInputFormat and KeyValueInputFormat class?
TextInputFormat: It reads lines of text files and provides the offset of the line as key to the Mapper and actual line as Value to the mapper.
KeyValueInputFormat: Reads text file and parses lines into key, Val pairs. Everything up to the first tab character is sent as key to the Mapper and the remainder of the line is sent as value to the mapper.

130.What is InputSplit in Hadoop?
When a Hadoop job is run, it splits input files into chunks and assign each split to a mapper to process. This is called InputSplit

131.How is the splitting of file invoked in Hadoop framework?
It is invoked by the Hadoop framework by running getInputSplit()method of the Input format class (like FileInputFormat) defined by the user.

132.Consider case scenario: In M/R system, - HDFS block size is 64 MB - Input format is FileInputFormat
- We have 3 files of size 64K, 65Mb and 127Mb
How many input splits will be made by Hadoop framework?
Hadoop will make 5 splits as follows:

- 1 split for 64K files

- 2 splits for 65MB files

- 2 splits for 127MB files

133.What is the purpose of RecordReader in Hadoop?
The InputSplit has defined a slice of work, but does not describe how to access it. The RecordReader class actually loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper. The RecordReader instance is defined by the Input Format.

134.After the Map phase finishes, the Hadoop framework does "Partitioning, Shuffle and sort". Explain what happens in this phase?
Partitioning: It is the process of determining which reducer instance will receive which intermediate keys and values. Each mapper must determine for all of its output (key, value) pairs which reducer will receive them. It is necessary that for any key, regardless of which mapper instance generated it, the destination partition is the same.

Shuffle: After the first map tasks have completed, the nodes may still be performing several more map tasks each. But they also begin exchanging the intermediate outputs from the map tasks to where they are required by the reducers. This process of moving map outputs to the reducers is known as shuffling.

Sort: Each reduce task is responsible for reducing the values associated with several intermediate keys. The set of intermediate keys on a single node is automatically sorted by Hadoop before they are presented to the Reducer.

135.If no custom partitioner is defined in Hadoop then how is data partitioned before it is sent to the reducer?
The default partitioner computes a hash value for the key and assigns the partition based on this result.

136.What are some typical functions of Job Tracker?
The following are some typical tasks of JobTracker:-

- Accepts jobs from clients

- It talks to the NameNode to determine the location of the data.

- It locates TaskTracker nodes with available slots at or near the data.

- It submits the work to the chosen TaskTracker nodes and monitors progress of each task by receiving heartbeat signals from Task tracker.

137.What is the relationship between Jobs and Tasks in Hadoop?
One job is broken down into one or many tasks in Hadoop.

138.Suppose Hadoop spawned 100 tasks for a job and one of the task failed. What will Hadoop do?
It will restart the task again on some other TaskTracker and only if the task fails more than four (default setting and can be changed) times will it kill the job.

139.Hadoop achieves parallelism by dividing the tasks across many nodes, it is possible for a few slow nodes to rate-limit the rest of the program and slow down the program. What mechanism Hadoop provides to combat this?
Speculative Execution.

140.How does speculative execution work in Hadoop?
JobTracker makes different TaskTrackers process same input. When tasks complete, they announce this fact to the JobTracker. Whichever copy of a task finishes first becomes the definitive copy. If other copies were executing speculatively, Hadoop tells the TaskTrackers to abandon the tasks and discard their outputs. The Reducers then receive their inputs from whichever Mapper completed successfully, first.

141.Using command line in Linux, how will you - See all jobs running in the Hadoop cluster
- Kill a job?
Hadoop job - list
Hadoop job - kill jobID

142.What is Hadoop Streaming?
Streaming is a generic API that allows programs written in virtually any language to be used as Hadoop Mapper and Reducer implementations.

143.What is the characteristic of streaming API that makes it flexible run MapReduce jobs in languages like Perl, Ruby, Awk etc.?
Hadoop Streaming allows to use arbitrary programs for the Mapper and Reducer phases of a MapReduce job by having both Mappers and Reducers receive their input on stdin and emit output (key, value) pairs on stdout.

144.What is Distributed Cache in Hadoop?
Distributed Cache is a facility provided by the MapReduce framework to cache files (text, archives, jars and so on) needed by applications during execution of the job. The framework will copy the necessary files to the slave node before any tasks for the job are executed on that node.

145.Benefits of distributed cache?
This is because distributed cache is much faster. It copies the file to all trackers at the start of the job. Now if the task tracker runs 10 or 100 Mappers or Reducer, it will use the same copy of distributed cache. On the other hand, if you put code in file to read it from HDFS in the MR Job then every Mapper will try to access it from HDFS hence if a TaskTracker run 100 map jobs then it will try to read this file 100 times from HDFS. Also HDFS is not very efficient when used like this.

146.What mechanism does Hadoop framework provide to synchronise changes made in Distribution Cache during runtime of the application?
none

147.Have you ever used Counters in Hadoop. Give us an example scenario?

148.Is it possible to provide multiple input to Hadoop? If yes then how can you give multiple directories as input to the Hadoop job?
Yes, the input format class provides methods to add multiple directories as input to a Hadoop job.

149.Is it possible to have Hadoop job output in multiple directories? If yes, how?
Yes, by using Multiple Outputs class.

150.What will a Hadoop job do if you try to run it with an output directory that is already present? Will it
- Overwrite it

- Warn you and continue

- Throw an exception and exit

The Hadoop job will throw an exception and exit.

151.How can you set an arbitrary number of mappers to be created for a job in Hadoop?
You can't set it

152.How can you set an arbitrary number of Reducers to be created for a job in Hadoop?
You can either do it programmatically by using method setNumReduceTasks in the Jobconf Class or set it up as a configuration setting.

153.How will you write a custom partitioner for a Hadoop job?
- Create a new class that extends Partitioner Class

- Override method getPartition

- In the wrapper that runs the Mapreduce, either

- Add the custom partitioner to the job programmatically using method set Partitioner Class or - add the custom partitioner to the job as a config file (if your wrapper reads from config file or oozie)

154.How did you debug your Hadoop code?
1. use counters
2. use the interface provided by hadoop web ui

155.Big Data?
An assortment of such a huge and complex data that it becomes very tedious to capture, store, process, retrieve, and analyze it with the help of on-hand database management tools or traditional processing techniques.

156.RDBMS and HADOOP?
RDBMS is useful when you want to seek one record from Big Data, whereas, Hadoop will be useful when you want Big Data in one shot and perform analysis on that later.

157.Why would any one set reducers to zero?
They normally are used to perform transformations on data without sorting and aggregations. (the steps between mapper and reducers are CPU intensive and thus disabling reducers to zero makes it fast)

158.Example of a command that creates an hadoop archive?
hadoop archive -archiveName myarchive.har -p / test_folder /myhome/myarchive

159.What are some concrete implementations of Output Format? 
DBOutputFormat
FileOutputFormat
NullOutputFormat
SequenceFileAsBinaryOutputFormat
TeraOutputFormat
TextOutputFormat

160.What does OutputFormat do?
1) Validate the output specification of the job. for e.g. check that the output directory doesnt already exist.

2) Provide the RecordWriter implementation to be used to write out the output files of the job.

161.What does InputFormat do?
1) Validate the input-specification of the job.

2) Split-up the input file(s) into logical InputSplits, each of which is assigned to an individual Mapper

3) Provide the RecordReader implementation to be used to glean input records from the logical InputSplit for processing by the Mapper

162.What are some concrete implementations of InputFormat?
CombineFileInputFormat
-DBInputFormat
-FileInputFormat
-KeyValueTextInputFormat
-NLineInputFormat
-SequenceFileAsBinaryInputFormat
-SequenceFileAsTextInputFormat
-StreamInputFormat
-TeraInputFormat
-TextInputFormat

163.What does RecordRreader do?
RecordReader, typically, converts the byte-orented view of the input provided by the InputSplit and presents a record-orented view for the Mapper and Reducer tasks for processing. 

It thus assumes the responsibility of processing record boundaries and presenting the tasks with keys and values

164. What are some concrete implementations of RecordReader?
DBInputFormat.DBRecordReader
-InnerJoinRecordReader
-KeyValueLineRecordReader
-OuterJoinRecordReader
-SequenceFileAsTextRecordReader
-SequenceFileRecordReader
-StreamBaseRecordReader
-StreamXmlRecordReader

165.How can counters be incremented in MapReduce jobs?
By calling the INCRCOUNTER method on the instance of REPORTER passed to the map or reduce method

166.What is a single way of running multiple jobs in order (Java) ?
JobClient.runJob(conf1);
JobClient.runJob(conf2);

167.What is the name of the distributed tool to retrieve logs, each daemon has source and sink, can also decorate (compress or filter) scabs out, master is point of configuration?
Flume


168. what is flume?
169.what is the use of batch size?can we set batchsize as zero?
ans:no we cant.it ll throw error mentioning batchsize cant nt be zero .

170.what is the use of rollsize and what is the default value?
to roll the file as per size . defalut value is 1024 bytes

171.what is the use of rollInterval and its default value?
it will roll the current file and write new file after the configured time .default vale is 30 second

172.waht is hdfs.rollcount and default value?
default value 10 .

173.how to desable rollcount,rollsize and rollInterval?
set value to zero

174.what is an agent in flume?

175.what is a source,sink and channel?

176.what is an event?

177.list all sources available in flume?

178.list all channels available in flume?

179.list all sinks available in flume?

180.what is interceptor?

181.
